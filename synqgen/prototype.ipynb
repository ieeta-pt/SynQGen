{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagoalmeida/safe_volume/A1-datasets/venv-310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from ni import HFNIEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    def gen():\n",
    "        with open(file_path) as f:\n",
    "            for data in map(json.loads, f):\n",
    "                yield data\n",
    "                \n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1052\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\", \n",
    "                                          padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if tokenizer.eos_token is None and tokenizer.pad_token is None:\n",
    "    raise RuntimeError(\"No avaialble padding token\")\n",
    "\n",
    "dataset = Dataset.from_generator(read_jsonl(\"../../KNOWLAM/clean_datasets/politics__speeches_clean.jsonl\"))\n",
    "#dataset = Dataset.from_generator(read_jsonl(\"../../KNOWLAM/clean_datasets/math__openai_grade_school_clean.jsonl\"))\n",
    "print(len(dataset))\n",
    "#dataset = Dataset.from_generator(read_jsonl(\"../../KNOWLAM/clean_datasets/math__openai_grade_school_clean.jsonl\"))\n",
    "#dataset = dataset.map(lambda sample: tokenizer(sample[\"text\"]),\n",
    "#                      remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class LMInput:\n",
    "    input_ids: List[int]\n",
    "    attention_mask: List[int]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "class MovingWindow:\n",
    "    \"https://stackoverflow.com/questions/64118654/best-way-to-implement-moving-window-in-python-for-loop\"\n",
    "    def __init__(self, tokens, window_size, step):\n",
    "        self.current = -step\n",
    "        self.last = len(tokens.input_ids) - window_size + 1\n",
    "        self.remaining = (len(tokens.input_ids) - window_size) % step\n",
    "        self.tokens = tokens\n",
    "        self.window_size = window_size\n",
    "        self.step = step\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self.current += self.step\n",
    "        if self.current < self.last:\n",
    "            return LMInput(input_ids=self.tokens.input_ids[self.current : self.current + self.window_size],\n",
    "                           attention_mask=self.tokens.attention_mask[self.current : self.current + self.window_size])\n",
    "        elif self.remaining:\n",
    "            self.remaining = 0\n",
    "            return LMInput(input_ids=self.tokens.input_ids[-self.window_size:],\n",
    "                           attention_mask=self.tokens.attention_mask[-self.window_size:])\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        \n",
    "def sliding_window(sample):\n",
    "    samples = {\"input_ids\": [], \"attention_mask\": [], \"id\":[]}\n",
    "    \n",
    "    #print(sample.keys())\n",
    "    \n",
    "    for i in range(len(sample[\"id\"])):\n",
    "        for j, s_sample in enumerate(MovingWindow(tokenizer(sample[\"text\"][i]), 2048, 1024)):\n",
    "            samples[\"input_ids\"].append(s_sample.input_ids)\n",
    "            samples[\"attention_mask\"].append(s_sample.attention_mask)\n",
    "            _id = sample[\"id\"][i]\n",
    "            samples[\"id\"].append(f\"{_id}_{j}\")\n",
    "            #samples[\"text\"].append(sample[\"text\"][i])\n",
    "\n",
    "    #print(samples)\n",
    "    return samples\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1052/1052 [00:23<00:00, 45.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(sliding_window, batched=True, batch_size=8, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.utils import PaddingStrategy\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ConvertToTensor:\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \n",
    "        samples = {\"input_ids\": [], \"attention_mask\": [], \"id\":[]}\n",
    "        \n",
    "        for feature in features:\n",
    "            samples[\"id\"].append(feature[\"id\"])\n",
    "            samples[\"input_ids\"].append(feature[\"input_ids\"])\n",
    "            samples[\"attention_mask\"].append(feature[\"attention_mask\"])\n",
    "        \n",
    "        samples[\"input_ids\"] = torch.as_tensor(samples[\"input_ids\"])\n",
    "        samples[\"attention_mask\"] = torch.as_tensor(samples[\"attention_mask\"])\n",
    "        \n",
    "        return samples\n",
    "\n",
    "\n",
    "dl = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=1, \n",
    "                                         collate_fn=ConvertToTensor(),\n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mb_sample)\u001b[38;5;241m.\u001b[39mlogits[:,context_tokens:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:] \u001b[38;5;66;03m# skip last\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     target_ids \u001b[38;5;241m=\u001b[39m \u001b[43mb_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m[:,context_tokens\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;66;03m# skip first + context  \u001b[39;00m\n\u001b[1;32m     12\u001b[0m     log_target_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(log_probs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, target_ids)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'input_ids'"
     ]
    }
   ],
   "source": [
    "context_tokens = 68\n",
    "\n",
    "for b_sample in dl:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "       \n",
    "        b_id = b_sample.pop(\"id\")\n",
    "        logits = model(**b_sample).logits[:,context_tokens:-1,:] # skip last\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "            \n",
    "        target_ids = b_sample.input_ids[:,context_tokens+1:, None].long() # skip first + context  \n",
    "        log_target_probs = torch.gather(log_probs, -1, target_ids).squeeze(-1)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [tensor([[10248,  6180,    13,  ...,  7030,   340,    13]])],\n",
       " 'attention_mask': [tensor([[1, 1, 1,  ..., 1, 1, 1]])]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = HFNIEstimator(\"EleutherAI/gpt-neo-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8792/8792 [00:06<00:00, 1458.25 examples/s]\n",
      "  0%|          | 3/8792 [00:00<27:50,  5.26it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(0), 'information': tensor(279.0507), 'seq_len': 83}\n",
      "{'id': tensor(1), 'information': tensor(288.3089), 'seq_len': 81}\n",
      "{'id': tensor(2), 'information': tensor(414.1760), 'seq_len': 134}\n",
      "{'id': tensor(3), 'information': tensor(439.0298), 'seq_len': 151}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/8792 [00:00<13:38, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(4), 'information': tensor(288.2872), 'seq_len': 91}\n",
      "{'id': tensor(5), 'information': tensor(513.2334), 'seq_len': 189}\n",
      "{'id': tensor(6), 'information': tensor(313.2731), 'seq_len': 119}\n",
      "{'id': tensor(7), 'information': tensor(518.6471), 'seq_len': 214}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/8792 [00:01<11:52, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(8), 'information': tensor(652.6773), 'seq_len': 198}\n",
      "{'id': tensor(9), 'information': tensor(874.0701), 'seq_len': 346}\n",
      "{'id': tensor(10), 'information': tensor(592.0710), 'seq_len': 208}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/8792 [00:01<09:04, 16.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(11), 'information': tensor(625.5753), 'seq_len': 207}\n",
      "{'id': tensor(12), 'information': tensor(363.7970), 'seq_len': 107}\n",
      "{'id': tensor(13), 'information': tensor(382.7283), 'seq_len': 142}\n",
      "{'id': tensor(14), 'information': tensor(274.9824), 'seq_len': 87}\n",
      "{'id': tensor(15), 'information': tensor(584.6282), 'seq_len': 203}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/8792 [00:01<08:44, 16.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(16), 'information': tensor(503.1963), 'seq_len': 137}\n",
      "{'id': tensor(17), 'information': tensor(664.3574), 'seq_len': 271}\n",
      "{'id': tensor(18), 'information': tensor(384.5544), 'seq_len': 197}\n",
      "{'id': tensor(19), 'information': tensor(444.6639), 'seq_len': 144}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/8792 [00:01<07:42, 18.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(20), 'information': tensor(407.0764), 'seq_len': 136}\n",
      "{'id': tensor(21), 'information': tensor(386.0840), 'seq_len': 154}\n",
      "{'id': tensor(22), 'information': tensor(375.9242), 'seq_len': 157}\n",
      "{'id': tensor(23), 'information': tensor(523.7769), 'seq_len': 173}\n",
      "{'id': tensor(24), 'information': tensor(440.4074), 'seq_len': 127}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 27/8792 [00:02<07:46, 18.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(25), 'information': tensor(506.9406), 'seq_len': 204}\n",
      "{'id': tensor(26), 'information': tensor(496.5264), 'seq_len': 133}\n",
      "{'id': tensor(27), 'information': tensor(694.4652), 'seq_len': 233}\n",
      "{'id': tensor(28), 'information': tensor(312.5536), 'seq_len': 107}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 32/8792 [00:02<07:39, 19.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(29), 'information': tensor(546.5273), 'seq_len': 218}\n",
      "{'id': tensor(30), 'information': tensor(481.1530), 'seq_len': 173}\n",
      "{'id': tensor(31), 'information': tensor(474.1942), 'seq_len': 168}\n",
      "{'id': tensor(32), 'information': tensor(493.3712), 'seq_len': 181}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 37/8792 [00:02<06:50, 21.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(33), 'information': tensor(430.3057), 'seq_len': 178}\n",
      "{'id': tensor(34), 'information': tensor(310.5411), 'seq_len': 100}\n",
      "{'id': tensor(35), 'information': tensor(352.9236), 'seq_len': 106}\n",
      "{'id': tensor(36), 'information': tensor(310.5780), 'seq_len': 116}\n",
      "{'id': tensor(37), 'information': tensor(505.1980), 'seq_len': 241}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 40/8792 [00:02<06:59, 20.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(38), 'information': tensor(395.6297), 'seq_len': 111}\n",
      "{'id': tensor(39), 'information': tensor(483.4924), 'seq_len': 162}\n",
      "{'id': tensor(40), 'information': tensor(580.9738), 'seq_len': 195}\n",
      "{'id': tensor(41), 'information': tensor(410.3804), 'seq_len': 146}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 46/8792 [00:02<07:15, 20.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(42), 'information': tensor(532.9274), 'seq_len': 171}\n",
      "{'id': tensor(43), 'information': tensor(380.1443), 'seq_len': 155}\n",
      "{'id': tensor(44), 'information': tensor(410.1493), 'seq_len': 158}\n",
      "{'id': tensor(45), 'information': tensor(450.1256), 'seq_len': 192}\n",
      "{'id': tensor(46), 'information': tensor(458.4406), 'seq_len': 181}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 52/8792 [00:03<06:49, 21.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(47), 'information': tensor(357.3480), 'seq_len': 129}\n",
      "{'id': tensor(48), 'information': tensor(291.4991), 'seq_len': 85}\n",
      "{'id': tensor(49), 'information': tensor(330.7055), 'seq_len': 90}\n",
      "{'id': tensor(50), 'information': tensor(524.4048), 'seq_len': 160}\n",
      "{'id': tensor(51), 'information': tensor(472.6688), 'seq_len': 187}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 55/8792 [00:03<06:45, 21.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(52), 'information': tensor(379.1021), 'seq_len': 125}\n",
      "{'id': tensor(53), 'information': tensor(322.9339), 'seq_len': 130}\n",
      "{'id': tensor(54), 'information': tensor(547.3682), 'seq_len': 178}\n",
      "{'id': tensor(55), 'information': tensor(292.4805), 'seq_len': 105}\n",
      "{'id': tensor(56), 'information': tensor(371.2491), 'seq_len': 165}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 61/8792 [00:03<07:08, 20.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(57), 'information': tensor(416.1475), 'seq_len': 211}\n",
      "{'id': tensor(58), 'information': tensor(583.3618), 'seq_len': 226}\n",
      "{'id': tensor(59), 'information': tensor(487.8830), 'seq_len': 169}\n",
      "{'id': tensor(60), 'information': tensor(487.3920), 'seq_len': 188}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 64/8792 [00:03<07:09, 20.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(61), 'information': tensor(618.9885), 'seq_len': 211}\n",
      "{'id': tensor(62), 'information': tensor(386.2302), 'seq_len': 122}\n",
      "{'id': tensor(63), 'information': tensor(398.8088), 'seq_len': 159}\n",
      "{'id': tensor(64), 'information': tensor(510.1176), 'seq_len': 168}\n",
      "{'id': tensor(65), 'information': tensor(399.2491), 'seq_len': 101}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 70/8792 [00:04<07:21, 19.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(66), 'information': tensor(687.6342), 'seq_len': 243}\n",
      "{'id': tensor(67), 'information': tensor(682.3404), 'seq_len': 274}\n",
      "{'id': tensor(68), 'information': tensor(548.3570), 'seq_len': 189}\n",
      "{'id': tensor(69), 'information': tensor(345.1340), 'seq_len': 98}\n",
      "{'id': tensor(70), 'information': tensor(250.2471), 'seq_len': 69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 73/8792 [00:04<07:17, 19.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(71), 'information': tensor(524.5659), 'seq_len': 149}\n",
      "{'id': tensor(72), 'information': tensor(608.3864), 'seq_len': 226}\n",
      "{'id': tensor(73), 'information': tensor(439.6553), 'seq_len': 135}\n",
      "{'id': tensor(74), 'information': tensor(326.5710), 'seq_len': 116}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 79/8792 [00:04<06:42, 21.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(75), 'information': tensor(367.9556), 'seq_len': 178}\n",
      "{'id': tensor(76), 'information': tensor(368.6167), 'seq_len': 103}\n",
      "{'id': tensor(77), 'information': tensor(308.9900), 'seq_len': 91}\n",
      "{'id': tensor(78), 'information': tensor(435.8450), 'seq_len': 125}\n",
      "{'id': tensor(79), 'information': tensor(279.6101), 'seq_len': 69}\n",
      "{'id': tensor(80), 'information': tensor(466.7167), 'seq_len': 146}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 85/8792 [00:04<06:53, 21.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(81), 'information': tensor(526.5134), 'seq_len': 231}\n",
      "{'id': tensor(82), 'information': tensor(433.8779), 'seq_len': 132}\n",
      "{'id': tensor(83), 'information': tensor(558.9106), 'seq_len': 210}\n",
      "{'id': tensor(84), 'information': tensor(408.6118), 'seq_len': 111}\n",
      "{'id': tensor(85), 'information': tensor(341.5211), 'seq_len': 109}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 90/8792 [00:05<08:10, 17.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor(86), 'information': tensor(306.8661), 'seq_len': 93}\n",
      "{'id': tensor(87), 'information': tensor(390.4664), 'seq_len': 110}\n",
      "{'id': tensor(88), 'information': tensor(283.0951), 'seq_len': 77}\n",
      "{'id': tensor(89), 'information': tensor(499.7301), 'seq_len': 158}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m estimator\u001b[39m.\u001b[39mni_from_generator(read_jsonl(\u001b[39m\"\u001b[39m\u001b[39m../../clean_datasets/math__openai_grade_school_clean.jsonl\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(r)\n",
      "File \u001b[0;32m~/safe_volume/A1-datasets/SynQGen/synqgen/ni.py:34\u001b[0m, in \u001b[0;36mNIEstimator.ni_from_generator\u001b[0;34m(self, generator, context_tokens)\u001b[0m\n\u001b[1;32m     29\u001b[0m target_ids \u001b[39m=\u001b[39m b_sample\u001b[39m.\u001b[39minput_ids[:,context_tokens\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:, \u001b[39mNone\u001b[39;00m]\u001b[39m.\u001b[39mlong() \u001b[39m# skip first + context  \u001b[39;00m\n\u001b[1;32m     30\u001b[0m log_target_probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mgather(log_probs, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, target_ids)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[39myield\u001b[39;00m {\n\u001b[1;32m     33\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: b_id[\u001b[39m0\u001b[39m],\n\u001b[0;32m---> 34\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minformation\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39m-\u001b[39mlog_target_probs[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mcpu(),\n\u001b[1;32m     35\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mseq_len\u001b[39m\u001b[39m\"\u001b[39m: b_sample\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m context_tokens\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     36\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for r in estimator.ni_from_generator(read_jsonl(\"../../clean_datasets/math__openai_grade_school_clean.jsonl\")):\n",
    "#    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġthe'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{v:k for k,v in tokenizer.vocab.items()}[262]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
